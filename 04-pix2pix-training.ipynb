{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training conditional GANs (cGANs): Pix2Pix\n",
        "\n",
        "This notebook implements the Pix2Pix conditional GAN as described in the paper:\n",
        "- Isola et al. (2018) Image-to-Image Translation with Conditional Adversarial Networks: https://arxiv.org/pdf/1611.07004.pdf\n",
        "\n",
        "Use the \"creating_a_pix2pix_dataset\" notebook to create a dataset, or alternatively use an existing dataset by downloading it from one of these links:\n",
        "-   Standard pix2pix datasets: [http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/)\n",
        "-   Comic faces: [https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic)\n",
        "-   Maps: [https://www.kaggle.com/datasets/alincijov/pix2pix-maps](https://www.kaggle.com/datasets/alincijov/pix2pix-maps)\n",
        "-   Edges to Rembrandt: [https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset](https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset)\n",
        "-   Depth [https://www.kaggle.com/datasets/greg115/pix2pix-depth](https://www.kaggle.com/datasets/greg115/pix2pix-depth)\n",
        "\n",
        "\n",
        "Now let's do our usual list of imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then proceed with setting up the notebook so it can find and parse our dataset and train the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path = \"./datasets/edge2comics/\" # Change this for your custom dataset\n",
        "target_index = 1 # This \n",
        "\n",
        "img_channels = 3 # Do not change\n",
        "img_size = 256   # Do not change \n",
        "batch_size = 1 \n",
        "val_size = 0.05  # Validation set size\n",
        "# Try chaning 'cpu' to 'mps' if using mac M1/M2, may speed up things\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "if device == 'mps':\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each training image in a standard pix2pix dataset consists of one imgage divided into two adjacent **source** and **target** images.\n",
        "The layout of the source and target may vary from training set to trainig set, so we provide a `target_index` flag the determines on which side the target is (`0` if on the left and `1` if on the right). Set this so the examples from the dataset appear with the source image to the left.\n",
        "\n",
        "The following code also **augments** the dataset by applying random uniform scaling (by upscaling and cropping) and random mirroring to the input output pairs. This should lead to a more stable model according to the original pix2pix paper. Finally the images ar normalized to the [-1,1] range as required by our GAN-based model.\n",
        "\n",
        "We will organize the dataset in batches of size `1`, as that is generally suggested for pix2pix models. That means that we will update the weights of the model for each image pair separately.\n",
        "\n",
        "Run the code below and examine the resulting example images. Then set the `target_index` variable to reflect the position of the target image. That is `target_index=0` if the target image is on the left and `target_index=1` if it is on the right.\n",
        "\n",
        "NOTE: visualization will break here on M1/M2 macs if you used `mps` for the device (https://github.com/pytorch/pytorch/issues/84523)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_jitter(input_image, target_image):\n",
        "    # Resizing to 286x286\n",
        "    resize_transform = transforms.Resize(size=(286, 286), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    input_image = resize_transform(input_image)\n",
        "    target_image = resize_transform(target_image)\n",
        "\n",
        "    # Random cropping back to 256x256\n",
        "    i, j, h, w = transforms.RandomCrop.get_params(input_image, output_size=(256, 256))\n",
        "    input_image = transforms.functional.crop(input_image, i, j, h, w)\n",
        "    target_image = transforms.functional.crop(target_image, i, j, h, w)\n",
        "\n",
        "    # Random mirroring\n",
        "    if np.random.uniform() < 0.5:\n",
        "        input_image = transforms.functional.hflip(input_image)\n",
        "        target_image = transforms.functional.hflip(target_image)\n",
        "\n",
        "    return input_image, target_image\n",
        "\n",
        "class Pix2PixImageDataset(Dataset):\n",
        "    def __init__(self, path, target_index):\n",
        "        super(Pix2PixImageDataset, self).__init__()\n",
        "        self.files = [os.path.join(path, f) for f in os.listdir(path) if '.jpg' in f or '.png' in f]\n",
        "        self.target_index = target_index\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        image = Image.open(path)\n",
        "        image = torchvision.transforms.ToTensor()(image)\n",
        "        w = image.shape[-1]\n",
        "        w = w // 2\n",
        "\n",
        "        if target_index == 0:\n",
        "            input_image = image[:, :, w:]\n",
        "            target_image = image[:, :, :w]\n",
        "        else:\n",
        "            target_image = image[:, :, w:]\n",
        "            input_image = image[:, :, :w]\n",
        "        # Jitter\n",
        "        input_image, target_image = random_jitter(input_image, target_image)\n",
        "        # Normalize\n",
        "        input_image = input_image*2 - 1\n",
        "        target_image = target_image*2 - 1\n",
        "        return input_image.to(device), target_image.to(device)\n",
        "\n",
        "\n",
        "train_dataset = Pix2PixImageDataset(dataset_path, target_index)\n",
        "val_dataset = Pix2PixImageDataset(dataset_path, target_index)\n",
        "\n",
        "# get length of the full dataset before split, and save it in idx\n",
        "num_train = len(train_dataset)\n",
        "\n",
        "# create an array of idx numbers for each element of the full dataset\n",
        "idx = list(range(num_train))\n",
        "#print(num_train, idx)\n",
        "\n",
        "# perform train / val split for data points\n",
        "train_indices, val_indices = train_test_split(idx, test_size=val_size, random_state=42)\n",
        "\n",
        "# override datasets to only be samples for each split\n",
        "train_dataset = Subset(train_dataset, train_indices)\n",
        "val_dataset = Subset(val_dataset, val_indices)\n",
        "\n",
        "# create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# get a batch of training images\n",
        "sample_batch = [torch.cat(image_pair, axis=-1)[0] for image_pair in list(train_loader)[:64]]\n",
        "\n",
        "# create a grid of images\n",
        "img_grid = vutils.make_grid(sample_batch, padding=2, normalize=True)\n",
        "\n",
        "# convert to NumPy and transpose dimensions for matplotlib\n",
        "img_grid_np = np.transpose(img_grid.detach().cpu().numpy(), (1, 2, 0))\n",
        "\n",
        "# plot the grid of images\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(img_grid_np)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build  the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pix2pix model is a conditional generative adversarial network (cGAN). A CGAN\n",
        "is a type of GAN model used for generating new data samples with specific\n",
        "attributes or characteristics. In a CGAN, both the generator and discriminator\n",
        "are *conditioned* on additional information, such as class labels, tags, or\n",
        "other types of metadata. The generator network takes in random noise as well as\n",
        "the conditional information as input and produces a new data sample that matches\n",
        "the desired attributes. The discriminator network, on the other hand, tries to\n",
        "distinguish between the generated samples and real samples based on both their\n",
        "visual appearance and the conditional information. For the case of a pix2pix\n",
        "model the network is conditioned on an image, which should be transformed into\n",
        "an output image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Differently from a DC-GAN, the generator of the pix2pix model is based on the\n",
        "[U-net](https://arxiv.org/abs/1505.04597) architecture. A U-net model is a CNN architecture that is typically used\n",
        "for image segmentation tasks. The name U-net derives from the architecture,\n",
        "which resembles the letter &ldquo;U&rdquo;. It consists of two main parts: an *encoder* and\n",
        "a *decoder*. The encoder part consists of a series of convolutional layers,\n",
        "which *decrease* the spatial dimensionality of the input image while increasing its\n",
        "depth (using `Conv2d` layers). This is followed by a bottleneck layer that extracts the most important\n",
        "features from the input image. The decoder part is a &ldquo;mirror image&rdquo; of the\n",
        "encoder. It consists of a series of layers that gradually *increase* the spatial\n",
        "dimensionality of the output, while decreasing its depth (using `ConvTranspose2d` layers). The output of each consecutive layer in the encoder is\n",
        "concatenated with the output of a corresponding layer in the decoder, which creates a \"U\" shape. This creates\n",
        "&ldquo;skip connections&rdquo; that help preserve spatial information and correclations and avoid information\n",
        "loss during the encoding and decoding process. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, size=4, stride=2, apply_batchnorm=True):\n",
        "        # Convolution-BatchNorm-ReLU\n",
        "        super().__init__() \n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, size, stride=stride, padding=1, bias=not apply_batchnorm)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) if apply_batchnorm else None\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.batchnorm is not None:\n",
        "            x = self.batchnorm(x)\n",
        "        x = self.leakyrelu(x)\n",
        "        return x\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, size=4, stride=2, apply_dropout=False):\n",
        "        # Convolution-BatchNorm-Dropout-ReLU\n",
        "        super().__init__() \n",
        "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, size, stride=stride, padding=1, bias=True)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout(0.5) if apply_dropout else None\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_transpose(x)\n",
        "        x = self.batchnorm(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_channels=3):\n",
        "        super().__init__() \n",
        "        # encoder:\n",
        "        # C64-C128-C256-C512-C512-C512-C512-C512\n",
        "        # decoder with skip (in/out):\n",
        "        # CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
        "        # CD512-CD512 -CD512 -C512 -C256 -C128-C64\n",
        "        \n",
        "        self.encoders = nn.ModuleList([\n",
        "            Downsample(3, 64, apply_batchnorm=False),\n",
        "            Downsample(64, 128),\n",
        "            Downsample(128, 256),\n",
        "            Downsample(256, 512),\n",
        "            Downsample(512, 512),\n",
        "            Downsample(512, 512),\n",
        "            Downsample(512, 512),\n",
        "            Downsample(512, 512, apply_batchnorm=False)\n",
        "        ])\n",
        "\n",
        "        self.decoders = nn.ModuleList([\n",
        "            Upsample(512, 512, apply_dropout=True),\n",
        "            Upsample(1024, 512, apply_dropout=True),\n",
        "            Upsample(1024, 512, apply_dropout=True),\n",
        "            Upsample(1024, 512),\n",
        "            Upsample(1024, 256),\n",
        "            Upsample(512, 128),\n",
        "        ])\n",
        "        self.last_decoder = Upsample(256, 64)\n",
        "        self.last = nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for i, down in enumerate(self.encoders):\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "        skips = skips[:-1][::-1]\n",
        "        for i, up in enumerate(self.decoders):\n",
        "            x = up(x) \n",
        "            x = torch.cat([x, skips[i]], dim=1)\n",
        "        x = self.last_decoder(x)\n",
        "        x = self.last(x)\n",
        "        return torch.tanh(x)\n",
        "\n",
        "generator = Generator().to(device)\n",
        "print(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discriminator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The discriminator in the pix2pix model is a convolutional &ldquo;PatchGAN classifier&rdquo;.\n",
        "It tries to classify if each **image patch** if it is real or not real. In the\n",
        "following decoder, each 30 x 30 image patch of the output classifies a 70 x 70\n",
        "portion of the input image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_channels=3):\n",
        "        # C64-C128-C256-C512\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.down1 = Downsample(image_channels*2, 64, 4, apply_batchnorm=False)\n",
        "        self.down2 = Downsample(64, 128, 4)\n",
        "        self.down3 = Downsample(128, 256, 4)\n",
        "        self.down4 = Downsample(256, 512, 4, stride=1)\n",
        "        self.last = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, inp, tar):\n",
        "        x = torch.cat([inp, tar], dim=1)\n",
        "        x = self.down1(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.down3(x)\n",
        "        x = self.down4(x)\n",
        "        x = self.last(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "discriminator = Discriminator().to(device)\n",
        "print(discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate some images before training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let&rsquo;s generate some images before training to see what the network will output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def numpy_image(x):\n",
        "    return np.transpose(x.detach().cpu().numpy(), (1, 2, 0))*0.5 + 0.5\n",
        "\n",
        "def generate_images(model, test_input, tar, fname=''):\n",
        "    prediction = model(test_input) #, training=True)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    if tar is not None:\n",
        "        display_list = [test_input[0], tar[0], prediction[0]]\n",
        "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "    else:\n",
        "        display_list = [test_input[0], prediction[0]]\n",
        "        title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "    for i in range(len(title)):\n",
        "        plt.subplot(1, len(title), i+1)\n",
        "        plt.title(title[i])\n",
        "        # Getting the pixel values in the [0, 1] range to plot.\n",
        "        plt.imshow(numpy_image(display_list[i]))\n",
        "        plt.axis('off')\n",
        "\n",
        "    if fname:\n",
        "        plt.savefig(fname)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "for example_input, example_target in list(train_loader)[:3]:\n",
        "    print(example_input.device, example_target.device)\n",
        "    generate_images(generator, example_input, example_target)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generator loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "-   The generator loss is a sigmoid cross-entropy loss of the generated images and an array of ones.\n",
        "-   The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
        "-   This allows the generated image to become structurally similar to the target image.\n",
        "-   The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper.\n",
        "\n",
        "Feel free to experiment with modifying the value of `LAMBDA` (if you have time to spare:))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop procedes by separately optimizing the discriminator and generator at each iteration. The procedure can be summarized as follows:\n",
        "- For each example input we use the Generator to generate an output.\n",
        "- Update the discriminator by:\n",
        "    -  (1) Feeding it the input image and the example target image to classify the ground truth (example) pair.\n",
        "    -  (2) Feeding it the input image together with the generated output to classify the generated pair.\n",
        "    -  Using these two outputs (1 and 2) to compute the discriminator loss and to update the discriminator parameters to minimize this loss. In order to update only the discriminator, when computing step (2) the generated image is \"detached\" (using the `.detach()` function) from the Torch computation graph, so that the gradients will not be \"frozen\" and not propagated back to the generator. \n",
        "- Update the generator by:\n",
        "    -  Computing (2) again with the updated discriminator but this time without detaching the generated image\n",
        "    -  Computing the generator loss by combining the classification loss computed for the discriminator and the [L1 distance](https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c) between the generated image and the target one and finally updating the parameters of the generator to minimize this loss.\n",
        "\n",
        "#### Discriminator loss\n",
        "The discriminator loss (`disc_loss`) consists of the average of two terms, a `real_loss` and a `fake_loss`:\n",
        "- The `real_loss` is the is a [binary cross-entropy loss](https://gombru.github.io/2018/05/23/cross_entropy_loss/) of the (discriminated) real images and an array of ones (since these are the real images). \n",
        "- The `fake_loss` is the is a binary cross-entropy loss of the (discriminated) fake images and an array of zeros (since these are the fake images). \n",
        "\n",
        "#### Generator loss\n",
        "While GANs learn a loss that adapts to the data, cGANs (as Pix2Pix) learn a structured loss that penalizes a possible structure that differs from the network output and the target image. As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004) the generator loss consists of two terms:\n",
        "\n",
        "-   Similarly to the discriminator case, the first term `fake_gan_loss` is a sigmoid cross-entropy loss of the (discriminated) generated images and an array of ones, i.e. considering the generated output as a real sample.\n",
        "-   The second term `dist_loss` quantifies the L1 distance, i.e. the mean absolute error (absolute value of differences), between the generated image and the target image. This allows the generated image to become structurally similar to the target image.\n",
        "-   These two terms are combined as `fake_gan_loss + LAMBDA * dist_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = 200\n",
        "LAMBDA = 100 # Weight of L1 loss in optimization \n",
        "save_interval = 1\n",
        "\n",
        "# Automatically create model path from dataset path, change this in case you want to customize name\n",
        "model_path = os.path.join(\"./models/\", os.path.basename(os.path.dirname(dataset_path))) \n",
        "\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "L1_loss = nn.L1Loss()\n",
        "\n",
        "def train_step(input_image, target_image):\n",
        "    # Generate output\n",
        "    gen_output = generator(input_image) \n",
        "\n",
        "    # ---- Update discriminator ----\n",
        "    disc_optimizer.zero_grad() # Clear gradients\n",
        "\n",
        "    # Classify real and fake patches \n",
        "    # Here we \"freeze\" generator gradients since we only optimize the discriminator\n",
        "    real_patch = discriminator(input_image, target_image)\n",
        "    fake_patch = discriminator(input_image, gen_output.detach())\n",
        "\n",
        "    # Compute loss for real/fake patches\n",
        "    # log(D(x,y)) + log(1 - D(x,G(x)))\n",
        "    real_class = torch.ones_like(real_patch).to(device)\n",
        "    fake_class = torch.zeros_like(fake_patch).to(device)\n",
        "\n",
        "    real_loss = BCE_loss(real_patch, real_class)\n",
        "    fake_loss = BCE_loss(fake_patch, fake_class)\n",
        "    disc_loss = (real_loss + fake_loss)/2\n",
        "\n",
        "    # Propagate gradients and perform gradient descent step\n",
        "    disc_loss.backward()\n",
        "    disc_optimizer.step()\n",
        "\n",
        "    # ---- Update generator ---- \n",
        "    gen_optimizer.zero_grad() # Clear gradients\n",
        "    # Classify fake samples, now considering generator gradients\n",
        "    fake_patch = discriminator(input_image, gen_output)\n",
        "    # Compute loss according to paper \n",
        "    # log(D(x,G(x))) + L1(y,G(x))\n",
        "    fake_gan_loss = BCE_loss(fake_patch, real_class)\n",
        "    dist_loss = L1_loss(gen_output, target_image)\n",
        "    gen_loss = fake_gan_loss + LAMBDA * dist_loss\n",
        "\n",
        "    # Propagate gradients and perform gradient descent step\n",
        "    gen_loss.backward()\n",
        "    gen_optimizer.step()\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "n = len(train_loader)\n",
        "for epoch in range(epochs):\n",
        "    batch_d_losses = []\n",
        "    batch_g_losses = []\n",
        "    for i, (input_image, target_image) in enumerate(train_loader):\n",
        "        gen_loss, disc_loss = train_step(input_image, target_image)\n",
        "        batch_d_losses.append(disc_loss.item())\n",
        "        batch_g_losses.append(gen_loss.item())\n",
        "        sys.stdout.write(\"\\r\" + \"Epoch %d - image %d of %d \" % (epoch+1, i+1, n) + \"[gen loss: %.4f | disc loss: %.4f]\" % (gen_loss.item(), disc_loss.item()))\n",
        "\n",
        "    g_losses.append(np.mean(batch_g_losses))\n",
        "    d_losses.append(np.mean(batch_d_losses))\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.title('Losses')\n",
        "    plt.plot(np.array(d_losses) * 40, label='Discriminator')\n",
        "    plt.plot(g_losses, label='Generator')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(model_path, \"losses.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "    if epoch % save_interval == 0:\n",
        "        print('\\nSaving epoch %d to %s' % (epoch+1, model_path))    \n",
        "        for j, (example_input, example_target) in enumerate(list(train_loader)[:3]):\n",
        "            generate_images(generator, example_input, example_target, fname=os.path.join(model_path, \"e%03d_generated_image_%d.png\" % (epoch+1, j+1)))\n",
        "            generator_scripted = torch.jit.script(generator) \n",
        "            generator_scripted.save(os.path.join(model_path, \"e%0d_generator.pt\" % (epoch+1)))\n",
        "            # The following saves only model parameters\n",
        "            #torch.save(generator.state_dict(), os.path.join(model_path, \"e%0d_generator.pth\" % (epoch+1)))\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally let's try generating some images using the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for example_input, example_target in list(val_loader)[:3]:\n",
        "    print(example_input.device, example_target.device)\n",
        "    generate_images(generator, example_input, example_target)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "org": null,
    "vscode": {
      "interpreter": {
        "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
