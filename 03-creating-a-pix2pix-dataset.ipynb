{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating your own pix2pix dataset\n",
        "=================================\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation requirements\n",
        "\n",
        "To run this and the following Pix2Pix notebooks you may need to install some new Python packages. To do so, open a terminal and first make sure your environment is active\n",
        "```\n",
        "conda activate dmlap\n",
        "```\n",
        "It is not obligatory, but since we only just recently tested these new dependencies, you may want to duplicate your current dmlap environment and use that instead \n",
        "with \n",
        "```\n",
        "conda create -n dmlap2 --clone dmlap\n",
        "```\n",
        "and then use the new `dmlap2` environment instead.\n",
        "With the environment active:\n",
        "```\n",
        "pip install face_recognition\n",
        "pip install pyglet\n",
        "```\n",
        "Then:\n",
        "```\n",
        "conda install conda-forge::cairo\n",
        "```\n",
        "Followed by:\n",
        "```\n",
        "conda install conda-forge::pycairo\n",
        "```\n",
        "\n",
        "If you have not done so already, you should also need to install the [py5canvas](https://github.com/colormotor/py5canvas) module. To do so use \n",
        "```\n",
        "pip install git+https://github.com/colormotor/py5canvas.git\n",
        "```\n",
        "\n",
        "**NOTE** there is a chance that the installation with pycairo might go wrong on Mac. If that is the case, you may need to do:\n",
        "```\n",
        "xcode-select --install\n",
        "``` \n",
        "From the command line.\n",
        "### Updating py5canvas\n",
        "If you already installed py5canvas from the previous examples, you will need to updated it to the latest version. To do so use \n",
        "```\n",
        "pip install --upgrade  --force-reinstall --no-deps git+https://github.com/colormotor/py5canvas.git\n",
        "```\n",
        "\n",
        "## Pix2pix datasets\n",
        "A pix2pix dataset consists of a series of image pairs. Each pair consists of a *source* (or input) image and *target* image. The Pix2Pix model learns the transformation from source to target images and, hopefully after sufficient training, it learns to transform images similar to the training sources into images similar to the training targets. The standard Pix2Pix implementation operates on input and output images with a size of 256x256 pixels. There are different ways in which a pix2pix training set may be organized. Here we adopt the convetion of source and target images are layed next to each other into single training images that are 512x256 pixels. The code below allows you to create such a training set from arbitrarily sized images. Let's begin by importing some necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Modules\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "from skimage import feature, filters\n",
        "import cv2\n",
        "import glob\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up \n",
        "The following code is designed to work with different kinds of inputs. It requires setting the following parameters:\n",
        "\n",
        "-   `target_path` defines where your **target** images are located.\n",
        "-   `source_path` defines where your **source** images are located, if you already have these. Otherwise, set this to an empty string `''`.\n",
        "-   `dataset_path` defines where your pix2pix dataset will be saved.\n",
        "-   `is_input_pix_to_pix` set this to `True` if the input dataset already consists of an source and target pairs merged into a single image. This will be the case if you want to modify the source for an existing pix2pix dataset. An example of this situation may be that we have a dataset that translates edges to images of faces and we want to modify the input so it consists of face landmarks. In this case we need to extract only the target.\n",
        "-   `target_index` indicates where the target will is located in the training set.  If the target image is to the left set it to (`0`) or to (`1`) if the target image is to the right.\n",
        "\n",
        "Note you will have to put exactly the path to your image directories here, this code does not recursively search for images. Also note that the most common use case for this system will be with you providing an dataset of targets (desired outputs) that you will process to create the corresponding inputs (e.g. with edge detection or finding face landmarks). In that case you should not worry about the `source_path` directory below.\n",
        "\n",
        "Here, by default we will load the &ldquo;Face 2 comics&rdquo; dataset. Download the dataset from [https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic](https://www.kaggle.com/datasets/defileroff/comic-faces-paired-synthetic), unzip, and place the `face2comics_v1.0.0_by_Sxela` subdirecory of the archive in the dataset directory relative to this notebook. This is already a &ldquo;pix2pix-friendly&rdquo; dataset consisting, however, of pairs of images that are separated. We will use the images to create an &ldquo;Edges to comics&rdquo; dataset, where we apply edge detection to a subset of the source images and leave the corresponding comic version unchanged.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "target_path = './datasets/face2comics_v1.0.0_by_Sxela/comics/'\n",
        "source_path = './datasets/face2comics_v1.0.0_by_Sxela/face/'  # Only used if we already have source image examples\n",
        "dataset_path = './datasets/edge2comics'\n",
        "max_images = 500\n",
        "is_input_pix_to_pix = False\n",
        "target_index = 1\n",
        "\n",
        "# Uncomment and adjust paths to perform face detection as the source\n",
        "# target_path = './datasets/edges2rembrandt'\n",
        "# source_path = ''\n",
        "# dataset_path = './datasets/landmarks2rembrandt'\n",
        "# max_images = 500\n",
        "# is_input_pix_to_pix = True\n",
        "# target_index = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above also contains a commented section with paths for the case in which you operate on an existing pix2pix dataset consisting of 512x256 images, and you want to replace the source with a custom one. Later in the code you will find a commented section that identifies face landmarks in the targets of the dataset and use the polylines of the face landmarks as a source. For this specific example to work, it is expected that you download the [\"rembrandt pix2pix dataset\"](https://www.kaggle.com/datasets/grafstor/rembrandt-pix2pix-dataset/code) and unzip the images into a \"edges2rembrandt\" folder inside the dataset folder relative to this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the images to process\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let&rsquo;s load our target images and, optionally, our source images if we have set the `source_path` directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_image(path):\n",
        "    w, h = (256, 256)\n",
        "    if is_input_pix_to_pix: # In case we are already loading a pix2pix image\n",
        "        w, h = (512, 256)\n",
        "    img = io.imread(path) #image.load_img(path, target_size=size)\n",
        "    img = transform.resize(img, (h, w), anti_aliasing=True)\n",
        "    # If we are loading a pix2pix dataset just extract the target\n",
        "    if is_input_pix_to_pix:\n",
        "        if target_index==0:\n",
        "            img = img[:,:h,:]\n",
        "        else:\n",
        "            img = img[:,h:,:]\n",
        "    return (img*255).astype(np.uint8)\n",
        "\n",
        "def load_images_in_path(path):\n",
        "    files = glob.glob(path + '/*')\n",
        "    images = []\n",
        "    if max_images:\n",
        "        n = len(files)\n",
        "        files = files[:max_images]\n",
        "        print('%d of %d images'%(len(files), n))\n",
        "    else:\n",
        "        print('%d images'%len(files))\n",
        "    for imgfile in tqdm(files): #, desc='Loading images in ' + path):\n",
        "        img = load_image(imgfile)\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "print('Loading targets')\n",
        "target_images = load_images_in_path(target_path)\n",
        "if source_path:\n",
        "    print('Loaded sources')\n",
        "    source_images = load_images_in_path(source_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a sources and targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the procedure is to create our source images. Usually this will be done by applying a **transormation** to the target. The network will then learn to \"translate\" between this transformed source and the target. Note that for the pix2pix model to learn something useful, we need to have some form of correlation between source and target pairs. The code below has a number of transformations already setup for you. These are:\n",
        "\n",
        "-   `apply_canny_cv2` Applies Canny edge detection to an image. This uses OpenCV to apply the edge detection filter. You can set two parameters (thresholds between 0 and 255) that will determine the result of the edge detection: `thresh1` and `thresh2`. Experiment with these values to adjust the results to your liking. Additional details can be seen [here](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de).\n",
        "-   `apply_canny_skimage` Also applies Canny edge detection to an image, but it uses[scikit-image](https://scikit-image.org) for the edge detection, which has different parameters to OpenCV. You can set one parameter, `sigma` that determines the number of edges. In general, a higher number will produce less edges. See [this](https://scikit-image.org/docs/stable/auto_examples/edges/plot_canny.html) for additional details.\n",
        "-   `apply_face_landmarks` Finds face landmarks in an image by using [face_recognition](https://pypi.org/project/face-recognition/) and uses the Canvas API to draw the landmark polygons. Note that this function will fail if the face detector cannot find a face in the image. The code is set up so the image won't be included in the generated dataset if face detection fails.\n",
        "-   `apply_nothing` Leaves an image unchanged.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_canny_cv2(img, thresh1=160, thresh2=250):\n",
        "    import cv2\n",
        "    invert = False\n",
        "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(grayimg, thresh1, thresh2)\n",
        "    if invert:\n",
        "        edges = cv2.bitwise_not(edges)\n",
        "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "def apply_canny_skimage(img, sigma=1.5):\n",
        "    import cv2\n",
        "    from skimage import feature\n",
        "    invert = False\n",
        "    grayimg = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    edges = (feature.canny(grayimg, sigma=sigma)*255).astype(np.uint8)\n",
        "    if invert:\n",
        "        edges = cv2.bitwise_not(edges)\n",
        "    return cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "def apply_face_landmarks(img, stroke_weight=2):\n",
        "    from py5canvas import canvas\n",
        "    import face_recognition\n",
        "    \n",
        "    c = canvas.Canvas(256, 256)\n",
        "    c.background(0)\n",
        "    landmarks = face_recognition.face_landmarks(img)\n",
        "\n",
        "    if not landmarks:\n",
        "        # print('Failed to find landmarks')\n",
        "        return None\n",
        "    c.stroke_weight(stroke_weight)\n",
        "    c.no_fill()\n",
        "    c.stroke(255)\n",
        "    for points in landmarks[0].values():\n",
        "        c.polyline(points)\n",
        "    return c.get_image()\n",
        "\n",
        "def apply_nothing(img):\n",
        "    return img\n",
        "\n",
        "# Used to assign the tranformations to source or target\n",
        "def transform_source(func):\n",
        "    \n",
        "    def transform_func(index):\n",
        "        return func(source_images[index])\n",
        "    return transform_func\n",
        "\n",
        "def transform_target(func):\n",
        "    def transform_func(index):\n",
        "        return func(target_images[index])\n",
        "    return transform_func\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "You select how to combine these functions and how to generate source and target image by assigning two variables `source_fun` and `target_fun`. We assign a transformation to them via a `transform_source` and `transform_target` functions that take one of the transformations above (or one you define) as a parameter. For example:\n",
        "```Python\n",
        "source_fun = transform_target(apply_canny_skimage)\n",
        "target_fun = transform_target(apply_nothing)\n",
        "```\n",
        "Means:\n",
        "- Create a new source image by applying `apply_canny_skimage` to the provided target image\n",
        "- Create a new target image by applying `apply_nothing` to the provided target image (that is leave it unchanged)\n",
        "\n",
        "Or \n",
        "```Python\n",
        "source_fun = transform_source(apply_canny_skimage)\n",
        "target_fun = transform_target(apply_nothing)\n",
        "```\n",
        "Means:\n",
        "- Create a new source image by applying `apply_canny_skimage` to the provided **source** image (assumes we secified one)\n",
        "- Create a new target image by applying `apply_nothing` to the provided target image (that is leave it unchanged)\n",
        "\n",
        "And so forth... \n",
        "By default, the code below takes the source images from the face2comics datasets and creates a new \"edges2comics\" dataset by applying edge detection to the original source images (simply images of faces). The commented section applies face detection to the target images, you can use that if you decide to build the \"landmarks2rembrandt\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "source_fun = transform_source(apply_canny_skimage)\n",
        "target_fun = transform_target(apply_nothing)\n",
        "\n",
        "# landmarks2rembrandt example\n",
        "# source_fun = transform_target(apply_face_landmarks)\n",
        "# target_fun = transform_target(apply_nothing)\n",
        "\n",
        "\n",
        "# Show an example for the transformation that creates the source, \n",
        "# we give the function the index of the image we want to process\n",
        "index = 1\n",
        "target = target_images[index]\n",
        "source = source_fun(index)\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(source)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(target)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the dataset!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have defined all these settings, we only need to apply the transformations and save our dataset. We loop through all the input source and target images, apply the transformationsand stitch the new source and targets into a single image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# target_index = 1 # You can redefine this if you wish for example to flip target and source\n",
        "\n",
        "num_images = max_images \n",
        "shuffle = False\n",
        "image_indices = list(range(len(target_images)))\n",
        "if shuffle:\n",
        "    random.shuffle(image_indices)\n",
        "if num_images != 0:\n",
        "    image_indices = image_indices[:num_images]\n",
        "\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "\n",
        "index = 1\n",
        "for i in tqdm(image_indices, desc='Saving dataset to ' + dataset_path):\n",
        "    target = target_fun(i)\n",
        "    source = source_fun(i)\n",
        "    if source is None:\n",
        "        # print('Failed to transform image %d of %d'%(i+1, len(image_indices)))\n",
        "        continue\n",
        "\n",
        "    # Concatenate images into one and save\n",
        "    if target_index==1:\n",
        "        combined = np.hstack([source, target])\n",
        "    else:\n",
        "        combined = np.hstack([target, source])\n",
        "    io.imsave(os.path.join(dataset_path, '%d.png'%(index)), combined)\n",
        "    index += 1\n",
        "\n",
        "print(\"Final dataset contains %d images\"%index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "org": null,
    "vscode": {
      "interpreter": {
        "hash": "1c544d3133b9d8c6f36fca025551af31afa9ef134259e7064ad6be0c15e6401c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
